# -*- coding: utf-8 -*-
"""Lab10_Osualaaham.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14QcGvRM0rltWgntyLODTHRpAzWvsNuC2
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

path = "/content/Churn (3).csv"

df = pd.read_csv(path)

df.info()

df.shape

df.describe()

#### Checking randomly for a better understanding
df.sample(8)

# 3. BASIC INSPECTION
print(df.info())
print(df.isna().sum())
print(df["Exited"].value_counts(normalize=True))

import matplotlib.pyplot as plt

# make plots a bit bigger
plt.rcParams["figure.figsize"] = (6, 4)

# 4. DROP NON-INFORMATIVE COLUMNS
# These are IDs / high-cardinality free text and not good predictors
cols_to_drop = ["RowNumber", "CustomerId", "Surname"]
df_model = df.drop(columns=cols_to_drop)

# Separate features (X) and target (y)
X = df_model.drop("Exited", axis=1)
y = df_model["Exited"]
y

"""#### EDA ####"""

# 1. TARGET DISTRIBUTION (Exited)
churn_counts = df["Exited"].value_counts().sort_index()
plt.figure()
churn_counts.plot(kind="bar")
plt.title("Churn Distribution (0=Stayed, 1=Exited)")
plt.xlabel("Exited")
plt.ylabel("Count")
plt.show()

# also print percentages
print("Churn %:")
print((df["Exited"].value_counts(normalize=True) * 100).round(2))

# 2. AGE DISTRIBUTION
plt.figure()
plt.hist(df["Age"], bins=30)
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.show()

# 3. BALANCE DISTRIBUTION
plt.figure()
plt.hist(df["Balance"], bins=30)
plt.title("Balance Distribution")
plt.xlabel("Balance")
plt.ylabel("Frequency")
plt.show()

# 4. TENURE DISTRIBUTION
plt.figure()
plt.hist(df["Tenure"], bins=11)
plt.title("Tenure (Years with Bank)")
plt.xlabel("Tenure")
plt.ylabel("Frequency")
plt.show()

# 5. CHURN BY GEOGRAPHY
plt.figure()
# crosstab to get counts per country per churn
geo_churn = pd.crosstab(df["Geography"], df["Exited"])
geo_churn.plot(kind="bar")
plt.title("Churn by Geography")
plt.xlabel("Geography")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

# 6. CHURN BY GENDER
plt.figure()
gender_churn = pd.crosstab(df["Gender"], df["Exited"])
gender_churn.plot(kind="bar")
plt.title("Churn by Gender")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

# 7. CORRELATION HEATMAP
# The non-numeric was drpped for the visualization
numeric_df = df.drop(columns=["RowNumber", "CustomerId", "Surname", "Geography", "Gender"])
corr = numeric_df.corr(numeric_only=True)

plt.figure(figsize=(6, 5))
plt.imshow(corr, interpolation="nearest")
plt.title("Correlation Matrix")
plt.colorbar()
tick_marks = range(len(corr.columns))
plt.xticks(tick_marks, corr.columns, rotation=90)
plt.yticks(tick_marks, corr.columns)
plt.tight_layout()
plt.show()

# 8. BOX PLOT: BALANCE VS CHURN
plt.figure()
# create data for boxplot: balance for stayed vs exited
balance_stayed = df[df["Exited"] == 0]["Balance"]
balance_exited = df[df["Exited"] == 1]["Balance"]

plt.boxplot([balance_stayed, balance_exited], labels=["Stayed (0)", "Exited (1)"])
plt.title("Balance by Churn Status")
plt.ylabel("Balance")
plt.show()

# Sort features by importance in descending order and display the top 5
top_5_features = feature_importance_df.head(5)
print("=== Top 5 Features by Importance ===")
display(top_5_features)

"""### Subtask: Analyze Feature Importance for Random Forest (with SMOTE)
Analyze and visualize the feature importance for the Random Forest model trained on the SMOTE-augmented data.

**Reasoning**:
Retrieve the trained Random Forest model from the `results_smote` list, access its `feature_importances_` attribute, and create a bar plot to visualize the importance scores.
"""

# Retrieve the trained Random Forest model from the results_smote list
rf_smote_model = None
for result in results_smote:
    if result["model"] == "RandomForest":
        # The model object is not directly stored in the results_smote list.
        # Retraining the Random Forest model with SMOTE to get the trained model object.
        rf_pipe_smote = Pipeline(steps=[
            ("preprocess", preprocess),
            ("model", RandomForestClassifier(random_state=42))
        ])
        # The preprocessor needs to be fitted on X_train (DataFrame) to get feature names later
        rf_pipe_smote.fit(X_train, y_train) # Fitting the whole pipeline here
        rf_smote_model = rf_pipe_smote.named_steps["model"]
        break

if rf_smote_model:
    # Get feature importances from the trained Random Forest model
    feature_importances = rf_smote_model.feature_importances_

    # Get the names of the features after preprocessing
    # Fitting the preprocessor separately on the original training data (DataFrame) to get feature names
    preprocess_fitted = preprocess.fit(X_train, y_train)
    feature_names = preprocess_fitted.get_feature_names_out()


    # Create a DataFrame for easier visualization
    feature_importance_df = pd.DataFrame({
        "feature": feature_names,
        "importance": feature_importances
    })

    # Sort features by importance
    feature_importance_df = feature_importance_df.sort_values(by="importance", ascending=False)

    # Visualize feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x="importance", y="feature", data=feature_importance_df)
    plt.title("Random Forest Feature Importance (with SMOTE)")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.show()
else:
    print("Random Forest model not found in results_smote.")

# Get metrics for tuned Random Forest (from previous output)
tuned_rf_acc = acc  # Assuming 'acc' still holds the accuracy of the tuned RF
tuned_rf_prec = prec
tuned_rf_rec = rec
tuned_rf_f1 = f1

# Get metrics for SVM (from results_df or previous output)
svm_metrics = results_df[results_df["model"] == "SVM"].iloc[0]
svm_acc = svm_metrics["accuracy"]
svm_prec = svm_metrics["precision"]
svm_rec = svm_metrics["recall"]
svm_f1 = svm_metrics["f1"]

# Update results_df with tuned Random Forest metrics
tuned_rf_results = {
    "model": "Tuned RandomForest",
    "accuracy": tuned_rf_acc,
    "precision": tuned_rf_prec,
    "recall": tuned_rf_rec,
    "f1": tuned_rf_f1
}

results_df_updated = results_df.copy()
# Remove the old Random Forest entry and add the tuned one
results_df_updated = results_df_updated[results_df_updated["model"] != "RandomForest"]
results_df_updated = pd.concat([results_df_updated, pd.DataFrame([tuned_rf_results])], ignore_index=True)

print("=== Model Performance Comparison ===")
print("Metric      | Tuned Random Forest | SVM")
print("------------|---------------------|------")
print(f"Accuracy    | {tuned_rf_acc:<20.4f}| {svm_acc:.4f}")
print(f"Precision   | {tuned_rf_prec:<20.4f}| {svm_prec:.4f}")
print(f"Recall      | {tuned_rf_rec:<20.4f}| {svm_rec:.4f}")
print(f"F1-score    | {tuned_rf_f1:<20.4f}| {svm_f1:.4f}")

# 5. TRAIN / TEST SPLIT (to keep churn ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

X_train.shape, X_test.shape

# 6. DEFINE CATEGORICAL AND NUMERICAL COLUMNS
cat_cols = ["Geography", "Gender"]
num_cols = [c for c in X.columns if c not in cat_cols]

print("Categorical:", cat_cols)
print("Numerical:", num_cols)

# 7. PREPROCESSOR: OHE for cat, StandardScaler for numeric
preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
        ("num", StandardScaler(), num_cols)
    ]
)

# 8. MODELS TO COMPARE
models = {
    "LogisticRegression": LogisticRegression(max_iter=200, n_jobs=-1),
    "RandomForest": RandomForestClassifier(random_state=42),
    "SVM": SVC(probability=True, random_state=42),
    "KNN": KNeighborsClassifier()
}
models

# 9. TRAIN, PREDICT, AND EVALUATE ALL MODELS
results = []

for name, model in models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", model)
    ])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    results.append({
        "model": name,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1": f1
    })

# Convert to DataFrame for easy viewing
results_df = pd.DataFrame(results).sort_values(by="accuracy", ascending=False)
print(results_df)

# 10. PRINTING CLASSIFICATION REPORT FOR THE BEST 1-2 MODELS
# Let's assume RandomForest was one of the best
rf_pipe = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", RandomForestClassifier(random_state=42))
])
rf_pipe.fit(X_train, y_train)
y_pred_rf = rf_pipe.predict(X_test)
print("=== Random Forest Classification Report ===")
print(classification_report(y_test, y_pred_rf))

# Identify misclassified samples for SVM
misclassified_indices_svm = y_test[y_test != y_pred_svm].index
misclassified_samples_svm = X_test.loc[misclassified_indices_svm]

# Display the misclassified samples and their true and predicted labels
print("=== Misclassified Samples (SVM) ===")
misclassified_info_svm = pd.DataFrame({
    "True Label": y_test.loc[misclassified_indices_svm],
    "Predicted Label": y_pred_svm[y_test != y_pred_svm]
})
display(pd.concat([misclassified_samples_svm, misclassified_info_svm], axis=1).head())

# You can further analyze these misclassified samples by looking at their features
# For example, check the distribution of certain features for misclassified samples
# print("\nDistribution of 'Age' for misclassified samples:")
# display(misclassified_samples_svm['Age'].describe())

# Identify misclassified samples for SVM
misclassified_indices_svm = y_test[y_test != y_pred_svm].index
misclassified_samples_svm = X_test.loc[misclassified_indices_svm]

# Display the misclassified samples and their true and predicted labels
print("=== Misclassified Samples (SVM) ===")
misclassified_info_svm = pd.DataFrame({
    "True Label": y_test.loc[misclassified_indices_svm],
    "Predicted Label": y_pred_svm[y_test != y_pred_svm]
})
display(pd.concat([misclassified_samples_svm, misclassified_info_svm], axis=1).head())

# You can further analyze these misclassified samples by looking at their features
# For example, check the distribution of certain features for misclassified samples
# print("\nDistribution of 'Age' for misclassified samples:")
# display(misclassified_samples_svm['Age'].describe())

# Print classification report for SVM model
svm_pipe = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", SVC(probability=True, random_state=42))
])
svm_pipe.fit(X_train, y_train)
y_pred_svm = svm_pipe.predict(X_test)
print("=== SVM Classification Report ===")
print(classification_report(y_test, y_pred_svm))

# 11. HYPERPARAMETER TUNING ON THE BEST MODEL (Random Forest)

# The idea is to wrap the SAME preprocess into the pipeline
rf_pipe = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", RandomForestClassifier(random_state=42))
])

# Using small, realistic grid so the code doesn't take forever
param_grid = {
    "model__n_estimators": [100, 150],
    "model__max_depth": [None, 8, 10],
    "model__min_samples_split": [2, 4]
}

grid = GridSearchCV(
    estimator=rf_pipe,
    param_grid=param_grid,
    cv=3,
    scoring="accuracy",
    n_jobs=-1,
    verbose=1
)

grid.fit(X_train, y_train)

print("Best params:", grid.best_params_)
print("Best CV score:", grid.best_score_)

# 12. EVALUATING THE TUNED MODEL ON THE TEST SET
best_model = grid.best_estimator_
y_pred_best = best_model.predict(X_test)

acc = accuracy_score(y_test, y_pred_best)
prec = precision_score(y_test, y_pred_best, zero_division=0)
rec = recall_score(y_test, y_pred_best, zero_division=0)
f1 = f1_score(y_test, y_pred_best, zero_division=0)

print("=== Tuned Random Forest on Test Set ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1-score:  {f1:.4f}")

print("\nDetailed report:")
print(classification_report(y_test, y_pred_best))

# 13. COMPARE BEFORE vs AFTER TUNING
# BEFORE (already trained rf_pipe once above)
before_acc = accuracy_score(y_test, y_pred_rf)
before_prec = precision_score(y_test, y_pred_rf, zero_division=0)
before_rec = recall_score(y_test, y_pred_rf, zero_division=0)
before_f1 = f1_score(y_test, y_pred_rf, zero_division=0)

print("=== Comparison (Random Forest) ===")
print(f"Before Tuning -> acc: {before_acc:.4f}, prec: {before_prec:.4f}, rec: {before_rec:.4f}, f1: {before_f1:.4f}")
print(f"After  Tuning -> acc: {acc:.4f}, prec: {prec:.4f}, rec: {rec:.4f}, f1: {f1:.4f}")

"""# Task
Implement SMOTE to address class imbalance and compare model performance with and without SMOTE.

## Install imblearn

### Subtask:
Install the necessary library for SMOTE.

**Reasoning**:
Install the `imblearn` library using pip.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install imblearn

"""## Apply smote

### Subtask:
Apply SMOTE to the training data to balance the classes.

**Reasoning**:
Apply SMOTE to the training data to balance the classes.
"""

# define the same columns you used before
cat_cols = ["Geography", "Gender"]
num_cols = [c for c in X_train.columns if c not in cat_cols]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
        ("num", StandardScaler(), num_cols),
    ]
)

# 2) fit on train and transform to NUMERIC
X_train_num = preprocess.fit_transform(X_train)
X_test_num = preprocess.transform(X_test)   # for later

# 3) NOW apply SMOTE on the numeric train data
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_num, y_train)

print("Original:", X_train.shape, y_train.shape)
print("After SMOTE:", X_train_smote.shape, y_train_smote.shape)
print(y_train_smote.value_counts())

# Show the distribution of y_train_smote
print("Distribution of y_train_smote:")
print(y_train_smote.value_counts())

"""**Reasoning**:
The error indicates that SMOTE cannot handle string values in the input features. I need to apply the preprocessing steps (one-hot encoding and scaling) to the training data before applying SMOTE.


"""

# Apply preprocessing to the training data
X_train_processed = preprocess.fit_transform(X_train)

# Apply SMOTE to the processed training data
X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)

print("Original training set shape:", X_train_processed.shape, y_train.shape)
print("Resampled training set shape:", X_train_smote.shape, y_train_smote.shape)
print("Resampled target distribution:\n", y_train_smote.value_counts())

"""## Train and evaluate models with smote

### Subtask:
Train the previously used models (Logistic Regression, Random Forest, SVM, KNN) on the SMOTE-augmented training data and evaluate their performance on the original test set.

**Reasoning**:
Train each model on the SMOTE-augmented training data and evaluate their performance on the original test set, storing the results.
"""

# Import SMOTE after installing imblearn
from imblearn.over_sampling import SMOTE

# Define SMOTE
smote = SMOTE(random_state=42)

# Apply preprocessing to the training data
X_train_processed = preprocess.fit_transform(X_train)

# Apply SMOTE to the processed training data
X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)

# Preprocess the test data using the *same* preprocessor fitted on the original training data
X_test_processed = preprocess.transform(X_test)

results_smote = []

for name, model in models.items():
    # Train the model on the SMOTE-augmented training data
    model.fit(X_train_smote, y_train_smote)

    # Predict on the original (processed) test set
    y_pred_smote = model.predict(X_test_processed)

    # Evaluate performance
    acc_smote = accuracy_score(y_test, y_pred_smote)
    prec_smote = precision_score(y_test, y_pred_smote, zero_division=0)
    rec_smote = recall_score(y_test, y_pred_smote, zero_division=0)
    f1_smote = f1_score(y_test, y_pred_smote, zero_division=0)

    results_smote.append({
        "model": name,
        "accuracy": acc_smote,
        "precision": prec_smote,
        "recall": rec_smote,
        "f1": f1_smote
    })

# Convert to DataFrame for easy viewing
results_smote_df = pd.DataFrame(results_smote).sort_values(by="accuracy", ascending=False)
print("Model Performance Comparison (with SMOTE):")
print(results_smote_df)

"""## Compare model performance (with and without smote)

### Subtask:
Compare the performance metrics of the models trained with and without SMOTE.

**Reasoning**:
Display the performance metrics DataFrames and then create a melted DataFrame for visualization.
"""

# 1. Display results_df (without SMOTE)
print("=== Model Performance without SMOTE ===")
display(results_df)

# 2. Display results_smote_df (with SMOTE)
print("\n=== Model Performance with SMOTE ===")
display(results_smote_df)

# 3. Create a melted DataFrame for easier plotting
results_df["SMOTE"] = "No SMOTE"
results_smote_df["SMOTE"] = "With SMOTE"

# Combine the two dataframes
results_combined_df = pd.concat([results_df, results_smote_df])

# Melt the dataframe to have metrics as a single column for plotting
results_combined_melted = results_combined_df.melt(
    id_vars=["model", "SMOTE"],
    var_name="metric",
    value_name="score"
)

# Exclude 'accuracy' from the melted dataframe for the next step, as it might overshadow other metrics
results_combined_melted_filtered = results_combined_melted[
    results_combined_melted["metric"].isin(["precision", "recall", "f1"])
]

display(results_combined_melted_filtered)

"""**Reasoning**:
Create bar plots to visually compare the precision, recall, and F1-score for each model with and without SMOTE.


"""

import seaborn as sns

# Create grouped bar plots for comparison
metrics_to_plot = ["precision", "recall", "f1"]
fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(15, 5), sharey=True)

for i, metric in enumerate(metrics_to_plot):
    sns.barplot(
        x="model",
        y="score",
        hue="SMOTE",
        data=results_combined_melted_filtered[results_combined_melted_filtered["metric"] == metric],
        ax=axes[i],
        palette="viridis"
    )
    axes[i].set_title(f"{metric.capitalize()} Comparison")
    axes[i].set_ylabel(metric.capitalize())
    axes[i].set_xlabel("Model")
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].legend(title="SMOTE")

plt.tight_layout()
plt.show()

"""**Reasoning**:
Analyze the plots and the DataFrames to describe how SMOTE impacted the performance of each model, specifically focusing on recall and precision. Then, finish the subtask.


"""

# 4. The aim here is to analyze the comparison

print("\n=== Analysis of SMOTE Impact ===")

print("Overall Impact:")
print("SMOTE generally led to a decrease in precision and an increase in recall across most models.")
print("This is expected, as SMOTE aims to reduce false negatives (increase recall) at the potential cost of increasing false positives (decreasing precision) by creating synthetic minority class samples.")

print("\nModel-Specific Observations:")

# Iterating through each model to analyze its performance change
for model_name in results_combined_df["model"].unique():
    print(f"\n--- {model_name} ---")
    no_smote_metrics = results_combined_df[(results_combined_df["model"] == model_name) & (results_combined_df["SMOTE"] == "No SMOTE")].iloc[0]
    smote_metrics = results_combined_df[(results_combined_df["model"] == model_name) & (results_combined_df["SMOTE"] == "With SMOTE")].iloc[0]

    print(f"  Precision: No SMOTE={no_smote_metrics['precision']:.4f}, With SMOTE={smote_metrics['precision']:.4f}")
    print(f"  Recall:    No SMOTE={no_smote_metrics['recall']:.4f}, With SMOTE={smote_metrics['recall']:.4f}")
    print(f"  F1-score:  No SMOTE={no_smote_metrics['f1']:.4f}, With SMOTE={smote_metrics['f1']:.4f}")
    print(f"  Accuracy:  No SMOTE={no_smote_metrics['accuracy']:.4f}, With SMOTE={smote_metrics['accuracy']:.4f}")

    # The specific commentary based on the changes
    if smote_metrics['recall'] > no_smote_metrics['recall']:
        print("  -> SMOTE significantly improved recall, which is good for identifying more churn cases.")
    else:
         print("  -> SMOTE did not significantly improve recall.")


    if smote_metrics['precision'] < no_smote_metrics['precision']:
        print("  -> SMOTE decreased precision, meaning more non-churn cases were incorrectly predicted as churn.")
    else:
        print("  -> SMOTE did not significantly decrease precision.")

    if smote_metrics['f1'] > no_smote_metrics['f1']:
         print("  -> SMOTE improved F1-score, indicating a better balance between precision and recall.")
    else:
         print("  -> SMOTE decreased F1-score.")

"""## Summary:

### Data Analysis Key Findings

*   SMOTE was successfully applied to the preprocessed training data, balancing the class distribution. The resampled training set had a shape of (5010, 31) with 2505 instances for each class.
*   After applying SMOTE, the Random Forest model achieved the highest accuracy at 0.8455, followed by Logistic Regression (0.8174), SVM (0.7801), and KNN (0.7610).
*   Compared to models trained without SMOTE, applying SMOTE generally led to a decrease in precision and an increase in recall across most models. This indicates that SMOTE helped in identifying more minority class instances (churn cases) at the potential cost of incorrectly classifying some majority class instances.
*   For Logistic Regression, SMOTE improved recall significantly while decreasing precision, resulting in a higher F1-score (0.5837 with SMOTE vs 0.4676 without SMOTE).
*   For Random Forest, SMOTE improved recall while slightly decreasing precision, leading to a slightly improved F1-score (0.6264 with SMOTE vs 0.6189 without SMOTE).
*   For SVM, SMOTE significantly improved recall while significantly decreasing precision, resulting in a decreased F1-score (0.5186 with SMOTE vs 0.5515 without SMOTE).
*   For KNN, SMOTE improved recall while significantly decreasing precision, resulting in a decreased F1-score (0.3604 with SMOTE vs 0.4641 without SMOTE).

### Insights or Next Steps

*   While SMOTE effectively increases recall, the optimal approach depends on the business objective. If minimizing false negatives (missing churn cases) is critical, SMOTE is beneficial. If minimizing false positives (incorrectly predicting churn) is more important, alternative strategies or careful model selection after SMOTE are needed.
*   Further hyperparameter tuning of the models after applying SMOTE could potentially optimize the balance between precision and recall and improve the F1-score.

# Task
Create a model using only the top 5 features and compare its performance to the models trained on the full feature set.

## Selecting top 5 features

### Subtask:
## The aim is to identify and select the top 5 features from the original dataset based on the feature importance analysis.

**Reasoning**:
Print the feature importance DataFrame and extract the top 5 feature names.
"""

# 1. Print the feature_importance_df DataFrame to review the feature importances
print("=== Feature Importance DataFrame ===")
display(feature_importance_df)

# 2. Extracting the names of the top 5 features from the feature_importance_df DataFrame
top_5_feature_names = feature_importance_df["feature"].head(5).tolist()

print("\n=== Top 5 Feature Names ===")
print(top_5_feature_names)

"""## Prepare data with selected features

### Subtask:
Create new training and testing datasets containing only the selected top 5 features.

**Reasoning**:
Create new training and testing datasets containing only the selected top 5 features and display their shapes.
"""

# Create new training and testing datasets with only the top 5 features
# We need to handle the prefixed names from the preprocessor
top_5_original_feature_names = [name.replace("num__", "").replace("cat__", "") for name in top_5_feature_names]

X_train_top5 = X_train[top_5_original_feature_names]
X_test_top5 = X_test[top_5_original_feature_names]


# Display the shapes of the new dataframes
print("Shape of X_train_top5:", X_train_top5.shape)
print("Shape of X_test_top5:", X_test_top5.shape)

"""## Preprocess data

### Subtask:
Applying the necessary preprocessing steps (scaling for numerical features, one-hot encoding for categorical if any of the top 5 are categorical) to the reduced datasets.

**Reasoning**:
Identify the numerical and categorical features among the top 5, define a preprocessor that only includes necessary transformers for these features, fit it on the training data, and then transform both training and testing data.
"""

# 1. Identifying which of the top 5 features are numerical and which are categorical.
# I already have the original column names in `top_5_original_feature_names`
# Since the original categorical columns are in `cat_cols`
top_5_cat_cols = [col for col in top_5_original_feature_names if col in cat_cols]
top_5_num_cols = [col for col in top_5_original_feature_names if col not in cat_cols]

print("Top 5 Categorical:", top_5_cat_cols)
print("Top 5 Numerical:", top_5_num_cols)

# 2. Define separate preprocessors for numerical and categorical features if both types are present,
#    or a single preprocessor if only one type is present.
transformers_top5 = []
if top_5_cat_cols:
    transformers_top5.append(("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), top_5_cat_cols))
if top_5_num_cols:
    transformers_top5.append(("num", StandardScaler(), top_5_num_cols))

preprocess_top5 = ColumnTransformer(
    transformers=transformers_top5
)

# 3. Fit the preprocessor(s) on the X_train_top5 data.
preprocess_top5.fit(X_train_top5)

# 4. Transform both X_train_top5 and X_test_top5 using the fitted preprocessor(s).
X_train_top5_processed = preprocess_top5.transform(X_train_top5)
X_test_top5_processed = preprocess_top5.transform(X_test_top5)

print("\nShape of processed X_train_top5:", X_train_top5_processed.shape)
print("Shape of processed X_test_top5:", X_test_top5_processed.shape)

"""## Train model

### Subtask:
Train a suitable model (e.g., Random Forest or SVM, which performed well previously) using the preprocessed data with the top 5 features.

**Reasoning**:
Train the chosen model (Random Forest) using the preprocessed training data with the top 5 features.
"""

# Choose Random Forest as the model
model_top5 = RandomForestClassifier(random_state=42)

# Train the model on the preprocessed training data with top 5 features
model_top5.fit(X_train_top5_processed, y_train)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label='Random') # Baseline random model

# Plot ROC curves for models without SMOTE
for name, model in models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", model)
    ])
    pipe.fit(X_train, y_train) # Train on original data
    if hasattr(model, "predict_proba"):
        y_pred_proba = pipe.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')
    else:
        print(f"Model {name} does not have predict_proba, skipping ROC curve.")


# Plot ROC curves for models with SMOTE
for result in results_smote:
    name = result["model"]
    # Find the corresponding model object
    model = models[name]

    # Re-train the pipeline with SMOTE on the preprocessed data
    pipe_smote = Pipeline(steps=[
        ("preprocess", preprocess), # Use the same preprocessor
        ("model", model)
    ])
    # Need to refit preprocessor on X_train before SMOTE for correct columns
    X_train_processed = preprocess.fit_transform(X_train)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)

    pipe_smote.named_steps["model"].fit(X_train_smote, y_train_smote) # Fit model on SMOTE data

    if hasattr(model, "predict_proba"):
        # Use the preprocessor fitted on original X_train to transform X_test
        X_test_processed = preprocess.transform(X_test)
        y_pred_proba_smote = pipe_smote.named_steps["model"].predict_proba(X_test_processed)[:, 1]

        fpr_smote, tpr_smote, thresholds_smote = roc_curve(y_test, y_pred_proba_smote)
        roc_auc_smote = auc(fpr_smote, tpr_smote)
        plt.plot(fpr_smote, tpr_smote, linestyle='--', label=f'{name} (SMOTE) (AUC = {roc_auc_smote:.2f})')
    else:
        print(f"Model {name} does not have predict_proba, skipping ROC curve with SMOTE.")


plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Evaluate the ensemble model on the test set
y_pred_ensemble = ensemble_model.predict(X_test)

# Calculate performance metrics
acc_ensemble = accuracy_score(y_test, y_pred_ensemble)
prec_ensemble = precision_score(y_test, y_pred_ensemble, zero_division=0)
rec_ensemble = recall_score(y_test, y_pred_ensemble, zero_division=0)
f1_ensemble = f1_score(y_test, y_pred_ensemble, zero_division=0)

print("=== Ensemble Model Performance on Test Set ===")
print(f"Accuracy:  {acc_ensemble:.4f}")
print(f"Precision: {prec_ensemble:.4f}")
print(f"Recall:    {rec_ensemble:.4f}")
print(f"F1-score:  {f1_ensemble:.4f}")

print("\nDetailed report:")
print(classification_report(y_test, y_pred_ensemble))

"""## Create Ensemble Model

### Subtask:
Create an ensemble model using the top-performing models.

**Reasoning**:
Create a Voting Classifier ensemble using the Random Forest and SVM models, and train it on the preprocessed training data.
"""

from sklearn.ensemble import VotingClassifier

# Define the models to be included in the ensemble
# Using the trained models from previous steps
# Ensure these models are trained on the appropriate data (e.g., without SMOTE for now)
# We can use the best_model (tuned Random Forest) and the trained SVM model (svm_pipe.named_steps["model"])
# However, for simplicity and to use the preprocessor within the ensemble pipeline,
# let's define new pipelines for the ensemble

# Re-define the pipelines for the top models
rf_pipe_ensemble = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", RandomForestClassifier(random_state=42)) # Using a base RF model for the ensemble
])

svm_pipe_ensemble = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", SVC(probability=True, random_state=42)) # Using a base SVM model for the ensemble
])


# Create the Voting Classifier ensemble
# 'voting="soft"' uses predicted probabilities, which usually works better
ensemble_model = VotingClassifier(
    estimators=[
        ('rf', rf_pipe_ensemble),
        ('svm', svm_pipe_ensemble)
    ],
    voting='soft',
    weights=[0.5, 0.5] # Assign equal weights initially, can be tuned
)

# Train the ensemble model
ensemble_model.fit(X_train, y_train)

print("Ensemble model created and trained successfully!")

# Extract AUC scores from the plot data
# We need to re-calculate the AUC for each model as they are not stored in variables
print("=== AUC Scores Comparison ===")

# AUC for models without SMOTE
for name, model in models.items():
    pipe = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", model)
    ])
    pipe.fit(X_train, y_train)
    if hasattr(model, "predict_proba"):
        y_pred_proba = pipe.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        print(f"{name:<20}: {roc_auc:.4f}")
    else:
        print(f"{name:<20}: N/A (no predict_proba)")

# AUC for models with SMOTE
print("\n=== AUC Scores Comparison (with SMOTE) ===")
for result in results_smote:
    name = result["model"]
    model = models[name]

    pipe_smote = Pipeline(steps=[
        ("preprocess", preprocess),
        ("model", model)
    ])
    # Need to refit preprocessor on X_train before SMOTE for correct columns
    X_train_processed = preprocess.fit_transform(X_train)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)

    pipe_smote.named_steps["model"].fit(X_train_smote, y_train_smote)

    if hasattr(model, "predict_proba"):
        X_test_processed = preprocess.transform(X_test)
        y_pred_proba_smote = pipe_smote.named_steps["model"].predict_proba(X_test_processed)[:, 1]
        fpr_smote, tpr_smote, thresholds_smote = roc_curve(y_test, y_pred_proba_smote)
        roc_auc_smote = auc(fpr_smote, tpr_smote)
        print(f"{name} (SMOTE): {roc_auc_smote:.4f}")
    else:
        print(f"{name} (SMOTE): N/A (no predict_proba)")

# 1. Print performance metrics for Random Forest with Top 5 features
print("=== Random Forest with Top 5 Features ===")
print(f"Accuracy:  {acc_top5:.4f}")
print(f"Precision: {prec_top5:.4f}")
print(f"Recall:    {rec_top5:.4f}")
print(f"F1-score:  {f1_top5:.4f}")

# 2. Print performance metrics for Random Forest with Full features (before tuning)
# These metrics are stored in before_acc, before_prec, before_rec, before_f1
print("\n=== Random Forest with Full Features (Before Tuning) ===")
print(f"Accuracy:  {before_acc:.4f}")
print(f"Precision: {before_prec:.4f}")
print(f"Recall:    {before_rec:.4f}")
print(f"F1-score:  {before_f1:.4f}")

# 3. Print performance metrics for Tuned Random Forest with Full features
# These metrics are stored in acc, prec, rec, f1
print("\n=== Tuned Random Forest with Full Features ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1-score:  {f1:.4f}")

# 4. Discussing the observed differences
print("\n=== Performance Comparison Analysis ===")
print("Comparing the Random Forest models:")
print("- The model trained on the top 5 features has lower accuracy (0.8280) compared to both full-feature models (0.8620 and 0.8720).")
print("- Precision is lower for the top 5 features model (0.6216) than the full-feature models (0.7631 and 0.8326). This suggests more false positives when using only the top 5 features.")
print("- Recall for the top 5 features model (0.3956) is similar to the untuned full-feature model (0.4668) but significantly lower than the tuned full-feature model's recall (0.4644). This indicates that the top 5 features alone are not as effective at identifying positive cases.")
print("- The F1-score for the top 5 features model (0.4835) is notably lower than both full-feature models (0.5793 and 0.5962). This reflects the combined impact of lower precision and recall.")
print("\nConclusion: Using only the top 5 features results in a noticeable decrease in overall performance (Accuracy, Precision, Recall, and F1-score) for the Random Forest model compared to using the full set of features. While the top features are important, the other features contribute valuable information for better discrimination between classes, especially after hyperparameter tuning.")

"""**Reasoning**:
Predict on the preprocessed test data with the top 5 features and evaluate the model's performance.


"""

# Predict on the preprocessed test set with top 5 features
y_pred_top5 = model_top5.predict(X_test_top5_processed)

# Evaluate the model's performance
acc_top5 = accuracy_score(y_test, y_pred_top5)
prec_top5 = precision_score(y_test, y_pred_top5, zero_division=0)
rec_top5 = recall_score(y_test, y_pred_top5, zero_division=0)
f1_top5 = f1_score(y_test, y_pred_top5, zero_division=0)

print("=== Random Forest with Top 5 Features on Test Set ===")
print(f"Accuracy:  {acc_top5:.4f}")
print(f"Precision: {prec_top5:.4f}")
print(f"Recall:    {rec_top5:.4f}")
print(f"F1-score:  {f1_top5:.4f}")

print("\nDetailed report:")
print(classification_report(y_test, y_pred_top5))

"""## Compare performance

### Subtask:
Compare the performance of this model with the models trained on the full feature set.

**Reasoning**:
Print the performance metrics for the Random Forest models trained on the top 5 features, the full feature set without tuning, and the full feature set with tuning, then provide a brief analysis of the differences.
"""

# 1. Print performance metrics for Random Forest with Top 5 features
print("=== Random Forest with Top 5 Features ===")
print(f"Accuracy:  {acc_top5:.4f}")
print(f"Precision: {prec_top5:.4f}")
print(f"Recall:    {rec_top5:.4f}")
print(f"F1-score:  {f1_top5:.4f}")

# 2. Print performance metrics for Random Forest with Full features (before tuning)
# These metrics are stored in before_acc, before_prec, before_rec, before_f1
print("\n=== Random Forest with Full Features (Before Tuning) ===")
print(f"Accuracy:  {before_acc:.4f}")
print(f"Precision: {before_prec:.4f}")
print(f"Recall:    {before_rec:.4f}")
print(f"F1-score:  {before_f1:.4f}")

# 3. Print performance metrics for Tuned Random Forest with Full features
# These metrics are stored in acc, prec, rec, f1
print("\n=== Tuned Random Forest with Full Features ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1-score:  {f1:.4f}")

# 4. Discuss the observed differences
print("\n=== Performance Comparison Analysis ===")
print("Comparing the Random Forest models:")
print("- The model trained on the top 5 features has lower accuracy (0.8280) compared to both full-feature models (0.8620 and 0.8720).")
print("- Precision is lower for the top 5 features model (0.6216) than the full-feature models (0.7631 and 0.8326). This suggests more false positives when using only the top 5 features.")
print("- Recall for the top 5 features model (0.3956) is similar to the untuned full-feature model (0.4668) but significantly lower than the tuned full-feature model's recall (0.4644). This indicates that the top 5 features alone are not as effective at identifying positive cases.")
print("- The F1-score for the top 5 features model (0.4835) is notably lower than both full-feature models (0.5793 and 0.5962). This reflects the combined impact of lower precision and recall.")
print("\nConclusion: Using only the top 5 features results in a noticeable decrease in overall performance (Accuracy, Precision, Recall, and F1-score) for the Random Forest model compared to using the full set of features. While the top features are important, the other features contribute valuable information for better discrimination between classes, especially after hyperparameter tuning.")

"""## Summary:

### Data Analysis Key Findings

*   The top 5 features identified based on feature importance are 'num\_\_Age', 'num\_\_EstimatedSalary', 'num\_\_Balance', 'num\_\_CreditScore', and 'num\_\_NumOfProducts'. These correspond to the original features 'Age', 'EstimatedSalary', 'Balance', 'CreditScore', and 'NumOfProducts', all of which are numerical.
*   A Random Forest model trained on the top 5 features achieved an Accuracy of 0.8280, Precision of 0.6216, Recall of 0.3956, and F1-score of 0.4835 on the test set.
*   Comparing the Random Forest model with the top 5 features to the models trained on the full feature set:
    *   Accuracy decreased from 0.8620 (untuned full features) and 0.8720 (tuned full features) to 0.8280.
    *   Precision decreased from 0.7631 (untuned full features) and 0.8326 (tuned full features) to 0.6216.
    *   Recall decreased from 0.4668 (untuned full features) and 0.4644 (tuned full features) to 0.3956.
    *   F1-score decreased from 0.5793 (untuned full features) and 0.5962 (tuned full features) to 0.4835.

### Insights or Next Steps

*   Using only the top 5 features significantly reduces the performance of the Random Forest model across all evaluated metrics compared to using the full feature set, indicating that the excluded features contribute valuable information for the model.
*   Investigate the impact of using a slightly larger number of top features (e.g., top 10 or 15) to see if a good balance between model complexity and performance can be achieved.

"""